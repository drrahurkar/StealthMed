{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee7b32b",
   "metadata": {},
   "source": [
    "\n",
    "# Drug Standardization Pipeline (MarketScan • FAERS • MPRINT → UMLS/RxNorm)\n",
    "\n",
    "This notebook scaffolds a reproducible pipeline to normalize drug names across three sources using **UMLS/RxNorm CUIs** as the canonical identifier.\n",
    "It supports exact and fuzzy matching, handles salts/esters, and preserves audit trails for review.\n",
    "\n",
    "**Outputs**\n",
    "- Per-source standardized tables with `{source}_standardized.csv`\n",
    "- Fuzzy match review queue: `fuzzy_review_candidates.csv`\n",
    "- Final joined summary: `final_joined.csv`\n",
    "\n",
    "> Tip: Run cells top-to-bottom. Fill in the configuration block first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fbcdce",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Required paths ---\n",
    "# Update these paths to your local environment as needed.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Source inputs\n",
    "MS_PATH = Path(\"/Users/rahurkar.1/Library/CloudStorage/OneDrive-TheOhioStateUniversityWexnerMedicalCenter/FAERS/drug_id_platform/ms_freq.csv\")\n",
    "FAERS_PATH = Path(\"/path/to/faers_unique_drugs.csv\")          # TODO: set\n",
    "MPRINT_PATH = Path(\"/path/to/mprint_unique_drugs.csv\")        # TODO: set  (ideally includes a CUI column)\n",
    "\n",
    "# UMLS / RxNorm inputs\n",
    "# Option A: CSV extracts of RXNCONSO (and optionally RXNSAT/RXNREL if you have them)\n",
    "# Provide at least RXNCONSO with columns: CUI, SAB, TTY, STR, CODE, RXCUI (if present)\n",
    "RXNCONSO_CSV = Path(\"/path/to/RXNCONSO_subset.csv\")           # TODO: set\n",
    "\n",
    "# Option B (advanced): connect to a local UMLS/RxNorm database via SQLAlchemy\n",
    "USE_SQL_BACKEND = False\n",
    "SQLALCHEMY_URL = \"postgresql+psycopg2://user:password@localhost:5432/umls\"  # if using a DB\n",
    "\n",
    "# Columns\n",
    "# Set the column in each source that holds the raw drug text. Can be auto-guessed if None.\n",
    "MS_DRUG_COL = None       # e.g., \"drug\" or \"drug_name\"\n",
    "FAERS_DRUG_COL = None\n",
    "MPRINT_DRUG_COL = None   # If MPRINT already has CUIs, set MPRINT_CUI_COL below\n",
    "\n",
    "MPRINT_CUI_COL = \"CUI\"   # if MPRINT includes CUIs\n",
    "\n",
    "# Matching thresholds\n",
    "FUZZY_STRONG = 90   # auto-accept\n",
    "FUZZY_REVIEW = 80   # send to review queue if in [FUZZY_REVIEW, FUZZY_STRONG)\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"/mnt/data/drug_standardization_outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Config loaded. Edit paths above as needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b2000",
   "metadata": {},
   "source": [
    "## 2) Imports & helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "# Install rapidfuzz if missing (works offline)\n",
    "try:\n",
    "    from rapidfuzz import process, fuzz\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Please install rapidfuzz: pip install rapidfuzz\") from e\n",
    "\n",
    "def _clean_str(s: str) -> str:\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).lower().strip()\n",
    "    # remove dosage forms / noise words frequently seen in free text; tweak as needed\n",
    "    noise = [\n",
    "        r\"\\btablet(s)?\\b\", r\"\\btab(s)?\\b\", r\"\\bcapsule(s)?\\b\", r\"\\bcap(s)?\\b\",\n",
    "        r\"\\binjection\\b\", r\"\\binj\\b\", r\"\\bsolution\\b\", r\"\\bsuspension\\b\",\n",
    "        r\"\\bsyrup\\b\", r\"\\bcream\\b\", r\"\\bointment\\b\", r\"\\bspray\\b\", r\"\\bdrops?\\b\",\n",
    "        r\"\\ber\\b\", r\"\\bir\\b\", r\"\\bxr\\b\", r\"\\bdr\\b\"\n",
    "    ]\n",
    "    for pat in noise:\n",
    "        s = re.sub(pat, \" \", s)\n",
    "    # unify punctuation/separators\n",
    "    s = re.sub(r\"[^\\w\\s/+-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# terms whose presence should not block matching to the base IN\n",
    "SALT_TERMS = [\n",
    "    \"hcl\", \"hydrochloride\", \"sodium\", \"potassium\", \"sulfate\", \"sulphate\",\n",
    "    \"phosphate\", \"acetate\", \"tartrate\", \"mesylate\", \"oxalate\", \"nitrate\",\n",
    "    \"succinate\", \"fumarate\", \"bitartrate\", \"bromide\", \"magnesium\", \"calcium\"\n",
    "]\n",
    "\n",
    "def _strip_salts(s: str) -> str:\n",
    "    toks = [t for t in s.split() if t not in SALT_TERMS]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def _split_combos(s: str) -> List[str]:\n",
    "    # split on common combo separators\n",
    "    parts = re.split(r\"\\s*[+/,&]\\s*|\\s+with\\s+\", s)\n",
    "    parts = [p.strip() for p in parts if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def auto_guess_drug_col(df: pd.DataFrame) -> Optional[str]:\n",
    "    candidates = [\"drug\", \"drug_name\", \"medication\", \"ingredient\", \"name\", \"generic_name\", \"brand_name\"]\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if lc in candidates or any(k in lc for k in [\"drug\", \"name\", \"med\", \"ing\"]):\n",
    "            return c\n",
    "    # fallback to first column if it looks text-y\n",
    "    first = df.columns[0]\n",
    "    if pd.api.types.is_string_dtype(df[first]):\n",
    "        return first\n",
    "    return None\n",
    "\n",
    "def load_source(path: Path, drug_col: Optional[str]) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if drug_col is None:\n",
    "        drug_col = auto_guess_drug_col(df)\n",
    "    if drug_col is None:\n",
    "        raise ValueError(f\"Could not auto-detect drug column in {path.name}. Please set the *_DRUG_COL config.\")\n",
    "    df = df.rename(columns={drug_col: \"raw_drug\"})\n",
    "    df = df[[\"raw_drug\"]].dropna().drop_duplicates().reset_index(drop=True)\n",
    "    df[\"clean_drug\"] = df[\"raw_drug\"].apply(_clean_str)\n",
    "    df[\"base_drug\"] = df[\"clean_drug\"].apply(_strip_salts)\n",
    "    return df\n",
    "\n",
    "def load_rxnconso_csv(path: Path) -> pd.DataFrame:\n",
    "    rx = pd.read_csv(path, dtype=str)\n",
    "    # keep RXNORM only\n",
    "    rx = rx[rx[\"SAB\"].str.upper() == \"RXNORM\"].copy()\n",
    "    # standard clean\n",
    "    rx[\"clean_str\"] = rx[\"STR\"].apply(_clean_str)\n",
    "    rx[\"base_str\"] = rx[\"clean_str\"].apply(_strip_salts)\n",
    "    # prefer relevant TTYs (IN = ingredient, PIN = precise ingredient, SCD/SBD = clinical/brand drug)\n",
    "    # We'll keep all but later prioritize IN\n",
    "    return rx\n",
    "\n",
    "def pick_preferred_term(group: pd.DataFrame) -> pd.Series:\n",
    "    # prioritize IN term if present; else any\n",
    "    order = {\"IN\": 1, \"PIN\": 2, \"SCD\": 3, \"SBD\": 4, \"GPCK\": 5, \"BPCK\": 6}\n",
    "    group[\"_prio\"] = group[\"TTY\"].map(order).fillna(9)\n",
    "    best = group.sort_values([\"_prio\", \"STR\"]).iloc[0]\n",
    "    return pd.Series({\"CUI\": best[\"CUI\"], \"Preferred_STR\": best[\"STR\"], \"Preferred_TTY\": best[\"TTY\"]})\n",
    "\n",
    "def exact_match(source_df: pd.DataFrame, rx: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # exact on clean_str, then on base_str\n",
    "    merged1 = source_df.merge(rx[[\"CUI\",\"TTY\",\"STR\",\"clean_str\",\"base_str\"]],\n",
    "                              left_on=\"clean_drug\", right_on=\"clean_str\", how=\"left\")\n",
    "    # fill remaining via base_str\n",
    "    need2 = merged1[merged1[\"CUI\"].isna()].copy()\n",
    "    fill2 = need2.merge(rx[[\"CUI\",\"TTY\",\"STR\",\"clean_str\",\"base_str\"]],\n",
    "                        left_on=\"base_drug\", right_on=\"base_str\", how=\"left\", suffixes=(\"\",\"_b\"))\n",
    "    exact = pd.concat([merged1[~merged1[\"CUI\"].isna()], fill2[~fill2[\"CUI\"].isna()]], ignore_index=True)\n",
    "    unmatched = pd.concat([merged1[merged1[\"CUI\"].isna()], fill2[fill2[\"CUI\"].isna()]], ignore_index=True)\n",
    "    exact = exact.drop_duplicates(subset=[\"raw_drug\",\"CUI\"])\n",
    "    return exact, unmatched[[\"raw_drug\",\"clean_drug\",\"base_drug\"]].drop_duplicates()\n",
    "\n",
    "def build_string_index(rx: pd.DataFrame) -> Tuple[List[str], Dict[str, List[Tuple[str,str]]]]:\n",
    "    # Index from string → list of (CUI, STR, TTY) to retrieve CUIs after fuzzy match\n",
    "    str_to_rows = {}\n",
    "    candidates = set(rx[\"clean_str\"]).union(set(rx[\"base_str\"]))\n",
    "    candidates = sorted([c for c in candidates if isinstance(c, str)])\n",
    "    for _, r in rx.iterrows():\n",
    "        for key in [r[\"clean_str\"], r[\"base_str\"]]:\n",
    "            if not isinstance(key, str):\n",
    "                continue\n",
    "            str_to_rows.setdefault(key, []).append((r[\"CUI\"], r[\"STR\"], r[\"TTY\"]))\n",
    "    return candidates, str_to_rows\n",
    "\n",
    "def fuzzy_match(unmatched_df: pd.DataFrame, rx: pd.DataFrame, strong: int=90, review: int=80) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    candidates, str_to_rows = build_string_index(rx)\n",
    "    results = []\n",
    "    reviews = []\n",
    "    for _, row in unmatched_df.iterrows():\n",
    "        q = row[\"clean_drug\"]\n",
    "        if not q:\n",
    "            continue\n",
    "        match = process.extractOne(q, candidates, scorer=fuzz.WRatio)\n",
    "        if match is None:\n",
    "            continue\n",
    "        match_str, score, _ = match\n",
    "        rows = str_to_rows.get(match_str, [])\n",
    "        df = pd.DataFrame(rows, columns=[\"CUI\",\"STR\",\"TTY\"])\n",
    "        if df.empty:\n",
    "            continue\n",
    "        chosen = pick_preferred_term(df)\n",
    "        out = {\n",
    "            \"raw_drug\": row[\"raw_drug\"],\n",
    "            \"query\": q,\n",
    "            \"matched_str\": match_str,\n",
    "            \"score\": score,\n",
    "            \"CUI\": chosen[\"CUI\"],\n",
    "            \"Preferred_STR\": chosen[\"Preferred_STR\"],\n",
    "            \"Preferred_TTY\": chosen[\"Preferred_TTY\"]\n",
    "        }\n",
    "        if score >= strong:\n",
    "            results.append(out)\n",
    "        else:\n",
    "            # anything below strong goes to review (including low scores)\n",
    "            reviews.append(out)\n",
    "\n",
    "    fm = pd.DataFrame(results)\n",
    "    rv = pd.DataFrame(reviews)\n",
    "    return fm, rv\n",
    "\n",
    "def consolidate_matches(exact_df: pd.DataFrame, fuzzy_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # normalize columns and pick one CUI per raw_drug (prefer exact)\n",
    "    exact_norm = exact_df.rename(columns={\"STR\": \"Preferred_STR\", \"TTY\": \"Preferred_TTY\"})\n",
    "    exact_norm = exact_norm[[\"raw_drug\",\"CUI\",\"Preferred_STR\",\"Preferred_TTY\"]].drop_duplicates()\n",
    "\n",
    "    if not fuzzy_df.empty:\n",
    "        fuzzy_pick = fuzzy_df.sort_values([\"raw_drug\",\"score\"], ascending=[True, False]).drop_duplicates(\"raw_drug\")\n",
    "        fuzzy_pick = fuzzy_pick[[\"raw_drug\",\"CUI\",\"Preferred_STR\",\"Preferred_TTY\"]]\n",
    "    else:\n",
    "        fuzzy_pick = pd.DataFrame(columns=[\"raw_drug\",\"CUI\",\"Preferred_STR\",\"Preferred_TTY\"])\n",
    "\n",
    "    combined = pd.concat([exact_norm, fuzzy_pick], ignore_index=True)\n",
    "    combined = combined.drop_duplicates(\"raw_drug\", keep=\"first\")\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ce51f",
   "metadata": {},
   "source": [
    "## 3) Load RxNorm / UMLS reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if USE_SQL_BACKEND:\n",
    "    from sqlalchemy import create_engine, text\n",
    "    engine = create_engine(SQLALCHEMY_URL)\n",
    "    query_sql = (\n",
    "        \"SELECT CUI, SAB, TTY, STR \"\n",
    "        \"FROM RXNCONSO \"\n",
    "        \"WHERE UPPER(SAB) = 'RXNORM'\"\n",
    "    )\n",
    "    with engine.connect() as con:\n",
    "        rx = pd.read_sql(text(query_sql), con)\n",
    "else:\n",
    "    rx = load_rxnconso_csv(RXNCONSO_CSV)\n",
    "\n",
    "print(f\"RxNorm terms loaded: {len(rx):,}\")\n",
    "rx.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5605a2",
   "metadata": {},
   "source": [
    "## 4) Load sources & clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ms = load_source(MS_PATH, MS_DRUG_COL)\n",
    "print(\"MarketScan unique drugs:\", len(ms))\n",
    "\n",
    "# Uncomment when paths set\n",
    "# faers = load_source(FAERS_PATH, FAERS_DRUG_COL)\n",
    "# print(\"FAERS unique drugs:\", len(faers))\n",
    "\n",
    "# mprint = load_source(MPRINT_PATH, MPRINT_DRUG_COL)  # if MPRINT has CUIs, we'll align later\n",
    "# print(\"MPRINT unique drugs:\", len(mprint))\n",
    "\n",
    "ms.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7b8da",
   "metadata": {},
   "source": [
    "## 5) Match each source to UMLS/RxNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_source_to_rxnorm(src_df: pd.DataFrame, source_name: str):\n",
    "    exact, unmatched = exact_match(src_df, rx)\n",
    "    fuzzy, review = fuzzy_match(unmatched, rx, strong=FUZZY_STRONG, review=FUZZY_REVIEW)\n",
    "    combined = consolidate_matches(exact, fuzzy)\n",
    "    combined[\"source\"] = source_name\n",
    "    return combined, review\n",
    "\n",
    "ms_map, ms_review = match_source_to_rxnorm(ms, \"marketscan\")\n",
    "print(\"MarketScan mapped:\", len(ms_map), \"| review queue:\", len(ms_review))\n",
    "\n",
    "# When ready for others:\n",
    "# faers_map, faers_review = match_source_to_rxnorm(faers, \"faers\")\n",
    "# mprint_map, mprint_review = match_source_to_rxnorm(mprint, \"mprint\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6651767",
   "metadata": {},
   "source": [
    "## 6) Save per-source standardized outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ms_out = ms.merge(ms_map.drop(columns=[\"source\"]), on=\"raw_drug\", how=\"left\")\n",
    "ms_out = ms_out.rename(columns={\n",
    "    \"raw_drug\": \"Drug\",\n",
    "    \"CUI\": \"UMLS_CUI\",\n",
    "    \"Preferred_STR\": \"Preferred_Term\",\n",
    "    \"Preferred_TTY\": \"Preferred_TTY\"\n",
    "})\n",
    "ms_out.to_csv(OUTPUT_DIR / \"marketscan_standardized.csv\", index=False)\n",
    "\n",
    "# pd.DataFrame().to_csv(OUTPUT_DIR / \"faers_standardized.csv\", index=False)   # uncomment once mapped\n",
    "# pd.DataFrame().to_csv(OUTPUT_DIR / \"mprint_standardized.csv\", index=False)  # uncomment once mapped\n",
    "\n",
    "# Consolidate all review queues\n",
    "review_all = ms_review.copy()\n",
    "# review_all = pd.concat([ms_review, faers_review, mprint_review], ignore_index=True)\n",
    "\n",
    "if not review_all.empty:\n",
    "    review_all.to_csv(OUTPUT_DIR / \"fuzzy_review_candidates.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\", [p.name for p in Path(OUTPUT_DIR).glob(\"*.csv\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e105732b",
   "metadata": {},
   "source": [
    "## 7) Join across sources on CUI (placeholder until FAERS/MPRINT mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5895b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Once FAERS/MPRINT maps exist, you can do something like:\n",
    "#\n",
    "# final = (\n",
    "#     ms_out[[\"UMLS_CUI\",\"Preferred_Term\"]]\n",
    "#     .merge(faers_out[[\"UMLS_CUI\"]].assign(FAERS_flag=1), on=\"UMLS_CUI\", how=\"outer\")\n",
    "#     .merge(mprint_out[[\"UMLS_CUI\"]].assign(MPRINT_flag=1), on=\"UMLS_CUI\", how=\"outer\")\n",
    "#     .fillna({\"FAERS_flag\":0, \"MPRINT_flag\":0})\n",
    "#     .drop_duplicates()\n",
    "# )\n",
    "# final.to_csv(OUTPUT_DIR / \"final_joined.csv\", index=False)\n",
    "#\n",
    "# For count-based summaries, merge in your per-source frequency tables before grouping.\n",
    "print(\"Join step scaffolded. Populate once FAERS/MPRINT standardized files are created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c7ead",
   "metadata": {},
   "source": [
    "## 8) QC summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def qc_report(mapped: pd.DataFrame, source_label: str) -> pd.DataFrame:\n",
    "    total = len(mapped)\n",
    "    matched = mapped[\"UMLS_CUI\"].notna().sum()\n",
    "    return pd.DataFrame({\n",
    "        \"source\":[source_label],\n",
    "        \"total_unique_drugs\":[total],\n",
    "        \"matched\":[matched],\n",
    "        \"match_rate\":[round(100*matched/total,2) if total else 0.0]\n",
    "    })\n",
    "\n",
    "qc_tables = []\n",
    "qc_tables.append(qc_report(ms_out, \"marketscan\"))\n",
    "# qc_tables.append(qc_report(faers_out, \"faers\"))\n",
    "# qc_tables.append(qc_report(mprint_out, \"mprint\"))\n",
    "\n",
    "qc = pd.concat(qc_tables, ignore_index=True)\n",
    "qc.to_csv(OUTPUT_DIR / \"qc_summary.csv\", index=False)\n",
    "qc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3705f",
   "metadata": {},
   "source": [
    "## 9) (Optional) Apply manual overrides for fuzzy review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you curate 'manual_overrides.csv' with columns: raw_drug, CUI\n",
    "# You can re-run consolidation with enforced mappings.\n",
    "\n",
    "OVERRIDES = OUTPUT_DIR / \"manual_overrides.csv\"\n",
    "if OVERRIDES.exists():\n",
    "    ov = pd.read_csv(OVERRIDES, dtype=str)\n",
    "    # example apply to MarketScan\n",
    "    if not ov.empty:\n",
    "        ms_out2 = ms_out.merge(ov, on=\"raw_drug\", how=\"left\", suffixes=(\"\",\"_OVR\"))\n",
    "        # prefer override CUI if present\n",
    "        ms_out2[\"UMLS_CUI\"] = ms_out2[\"CUI\"].fillna(ms_out2[\"UMLS_CUI\"])\n",
    "        ms_out2 = ms_out2.drop(columns=[\"CUI\"])\n",
    "        ms_out2.to_csv(OUTPUT_DIR / \"marketscan_standardized_overridden.csv\", index=False)\n",
    "        print(\"Applied overrides → marketscan_standardized_overridden.csv\")\n",
    "else:\n",
    "    print(\"No overrides found. To use, create:\", OVERRIDES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280251f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Next steps\n",
    "1. Point `FAERS_PATH`, `MPRINT_PATH`, and `RXNCONSO_CSV` to your local files.\n",
    "2. Run sections 3 → 9.\n",
    "3. Review `fuzzy_review_candidates.csv`, curate `manual_overrides.csv` as needed, and re-run section 9.\n",
    "4. Add frequency columns and build the `final_joined.csv` summary for your app.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}